**A Special Case of Nonlinear Extrapolation Under ReLU and the Neural Tangent Kernel**
[Click here to view the paper (PDF)](paper.pdf)

*Author(s)*: Abiel J. Kim

*Date*: November 2025

*Keywords*: Machine Learning, Neural Network, Extrapolation, Neural Tangent Kernel, Pseudo-Inverse, Moore-Penrose Inverse, Asymptotic Analysis, Distributional Derivative, Tikhonov Regularization, Ridge Regression, Training Data, ReLU Activation, Squared Loss Error

**ABSTRACT**

It has been demonstrated both theoretically and empirically that the ReLU MLP tends to extrapolate linearly for an out-of-distribution evaluation point. The machine learning literature provides ample analysis with respect to the mechanisms to which linearity is induced. However, the analysis of extrapolation at the origin under the NTK regime remains a more unexplored special case. In particular, the infinite-dimensional feature map induced by the neural tangent kernel is not translationally invariant. This means that the study of an out-of-distribution evaluation point very far from the origin is not equivalent to the evaluation of a point very near the origin. And since the feature map is rotation invariant, these two special cases may represent the most canonically extreme bounds of ReLU NTK extrapolation. Ultimately, it is this loose recognition of the two special cases of extrapolation that motivate the discovery of quadratic extrapolation for an evaluation close to the origin.

**INTRODUCTION**

The work of \textcite{DBLP:journals/corr/abs-2009-11848} proves that an over-parameterized ReLU-activated multilayer perceptron (MLP) will extrapolate linearly when evaluated along any direction very distant from the origin. They formally prove extrapolative linearity by analysis of the learned regressor's functional form in the \textit{neural tangent kernel} (NTK) \textit{reproducing kernel hilbert space} (RKHS) \parencite{DBLP:journals/corr/abs-1806-07572}. And, since the infinite dimensional feature map induced by the neural tangent kernel is rotation invariant, the analysis covers the generalizable case of an evaluation point very distant from the origin. However, it is not difficult to recognize that the same feature map is not translation invariant. It is by a geometric reasoning that the origin of the RKHS must be a distinct special case whose analysis departs from Theorem 1 of \textcite{DBLP:journals/corr/abs-2009-11848}. That is, in the limit of a large relative distance between the training point set and the evaluation point, one observes that there must be two special locations of the evaluation point with respect to the NTK induced feature map: A location casted along a singular feature direction, and a location which intersects all feature directions.

It is this recognition of the distinguishable cases that motivates the extrapolative analysis at the origin location. The non translation invariance of the feature map implies that the extrapolative analysis at the origin and far from origin are not equivalent problems. It can be reasoned that they are two canonical cases of a more complete analysis of extrapolation. However, inducing extrapolation at the origin must be done carefully to ensure that the evaluation data is pushed out of the support of the training distribution space. This is achieved by this paper's definition of a labeled training set, which is formally presented in the problem setup of section 2. The desired effect of said definition is to induce a problem setup where all members of the training set are sent infinitely far away from the origin whilst fixing the evaluation data at the origin. Under this variant setting, we state Theorem 1, which discovers that an over-parameterized neural network extrapolates quadratically when evaluated near the origin. This finding contrasts, but does not conflict with, \textcite{DBLP:journals/corr/abs-2009-11848}, which contrastingly concerns itself an evaluation point far from the origin.

The paper is organized as follows. The proof of Theorem 1 is presented in §A.4 and will depend on the results of Lemmas 1 and 2, which are proven with continuity in §A.2 and §A.3 respectively. Our problem setup induces a special case of the NTK gram matrix which must be studied in §A.1 to set the stage for the remainder of the mathematics.
